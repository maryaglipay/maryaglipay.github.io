[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "maryaglipay.github.io",
    "section": "",
    "text": "This is a Quarto website. Hello!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/blog1/blog10.html",
    "href": "posts/blog1/blog10.html",
    "title": "What is an interrupted time series analysis?",
    "section": "",
    "text": "“In its simplest form, an ITS is modelled using a regression model (such as linear, logistic, or Poisson) that includes only three time based covariates, whose regression coefficients estimate the pre-intervention slope, the change in level at the intervention point, and the change in slope from pre-intervention to post-intervention” Kontopantelis et al. (2015)\n\\(logit(Y)=B_0 + B_1 time + B_2 after + B_3time*after\\)\n\\(B_1\\) is interpreted as the pre-intervention slope\n\\(B_2\\) is interpreted as the jump in Y after the interruption\n\\(B_3\\) is interpreted as the post-intervention slope\n\nStrengths of the interrupted time series design\n\nCompared to pre-post designs: Interrupted time series analyses can take into account the pattern, or slope, of the trend before and after the intervention and determine whether the was a real difference. For example, Penfold and Zhang (2013) uses the example of an intervention to prevent medication errors. See Figure 1 below. If we were just to use a pre-post analysis, we might erroneously make the conclusion that the intervention caused a change in the medication error rate. However, if we examine the slope, it appears that the trend of medication error was downward anyway, and the intervention did little to change that trend.\n\n\n\nEase of interpretation. The graphs that come out of an ITS is generally well-understood\n\n\n\nLimitations of Interrupted Time Series Design\n\nPreferably 8 or 9 measures on each side of the interruption\nMore likely to be impacted by cyclical trends (pre and post periods should be matched on cycle) - this is especially key for things that influenza. We should try to compare NPI slopes within the same season\n\n\n\nComparison with D-I-D Design\nD-I-D design measures the change in outcome pre and post intervention for an exposed group and a control group, and then finds these diffferences to see the “difference in differences” (see Warton (n.d.)).\n\n\n\n\n\nReferences\n\nKontopantelis, E., T. Doran, D. A. Springate, I. Buchan, and D. Reeves. 2015. “Regression Based Quasi-Experimental Approach When Randomisation Is Not an Option: Interrupted Time Series Analysis.” BMJ 350 (jun09 5): h2750–50. https://doi.org/10.1136/bmj.h2750.\n\n\nPenfold, Robert B., and Fang Zhang. 2013. “Use of Interrupted Time Series Analysis in Evaluating Health Care Quality Improvements.” Academic Pediatrics, Quality Improvement in Pediatric Health Care, 13 (6, Supplement): S38–44. https://doi.org/10.1016/j.acap.2013.08.002.\n\n\nWarton, E Margaret. n.d. “Time After Time: Difference-in-Differences and Interrupted Time Series Models in SAS®,” 21."
  },
  {
    "objectID": "posts/blog1/blog11.html",
    "href": "posts/blog1/blog11.html",
    "title": "But wait, isn’t the effectiveness of masks known?",
    "section": "",
    "text": "It’s a good question! Isn’t the effectiveness of masks already known? Why do we have to use all these fancy methods, why did we collect all of this data if we already know?\nFirst, what I want to do is recognize the many studies out there who have provided evidence for mask wearing- they have really guided many the public health recommendations today.\nOur goal is to provide a measure of the impact of individual-level behaviour on individual level risk for children over the course of the pandemic. And this is where the evidence becomes very very sparse.\nThere has been, for example, a well-designed cluster randomized trial (see Abaluck et al. (2021)) in Bangladesh that found that a mask intervention that sought to provide and educate villages about masks saw a 9% reduced seroprevalence of infection compared to villages without the intervention. However, this was done in adults, in an entirely different context.\nThere have been several studies looking at community-wide implementation or removal of mask mandates, for example, a very recent one looked at the removal of masking policy in Massachusetts school districts, and found that the lifting of masking requirements was associated with an additional 44.9 Covid cases per 1000 students and staff (see Cowger et al. (2022)) . That’s pretty convincing. But again, this is a community-wide intervention, it doesn’t quite get at the impact of individual level action on individual risk.\nAlso, this study, and many other studies looking looking at the addition of or removal of a mandate are really just looking at a single point in time. My goal is to to examine the impact of individual level adherence to masks on individual risk in children over the span of the entire pandemic. This longitudinal nature of this study is important because 1) it helps us to establish temporality, which is unavailable in cross sectional studies and 2) the nature of the pandemic, people’s appetite toward wearing masks and adhering to NPIs has changed over time. I think to increase applicability of the finding,\n\n\n\n\nReferences\n\nAbaluck, Jason, Laura H. Kwong, Ashley Styczynski, Ashraful Haque, Md. Alamgir Kabir, Ellen Bates-Jefferys, Emily Crawford, et al. 2021. “Impact of Community Masking on COVID-19: A Cluster-Randomized Trial in Bangladesh.” Science 375 (6577): eabi9069. https://doi.org/10.1126/science.abi9069.\n\n\nCowger, Tori L., Eleanor J. Murray, Jaylen Clarke, Mary T. Bassett, Bisola O. Ojikutu, Sarimer M. Sánchez, Natalia Linos, and Kathryn T. Hall. 2022. “Lifting Universal Masking in Schools  Covid-19 Incidence Among Students and Staff.” New England Journal of Medicine 387 (21): 1935–46. https://doi.org/10.1056/NEJMoa2211029."
  },
  {
    "objectID": "posts/blog1/blog13.html",
    "href": "posts/blog1/blog13.html",
    "title": "Consequences of loss to follow-up",
    "section": "",
    "text": "One problem cohort studies often face is loss to follow up. And often times the processes that give rise to participation in the cohort in the first place are also the processes that keep participants in the study. So, like we see in other cohort studies, our study tends to skew toward whiter, more educated, and more affluent families. And although I haven’t seen a lot of the data, I imagine that those who remain in the study tend to skew that way too. How does this impact my study?\n\nLet’s look at this figure, which can be found in @hernán. If we imagine that U is having low income. Low income may mean less time to complete surveys, which means censoring. People with low income are also at high risk of Covid. There will be bias because we have conditioned on C, i.e. we are only looking at participants who remained in the study.\nThe nice thing about time to event analyses is that participants who contribute data to the study but drop out are still considered in all of the analyses. But they are considered MAR, or missing at random, which means that the missingness is unrelated to any of the variables in the dataset.\nWhen we know that’s not true, we’ll have to look at other tools. In the book Hernán and Robins (n.d.), he mentions using inverse probability of censoring weighting to account for selection bias. Essentially, you upweight participants who, based on their propensity to drop out, are more likely to drop out. Given the assumptions of exchangeability, this participant is used to represent participants like them that would have actually dropped out.\nIn the parametric g-formula, there are also ways, which I am still learning about. But essentially what it involves is simulating outcomes beyond the time that they were actually censored. Basically, you simulate data as though they had not been censored.\n\n\n\n\nReferences\n\nHernán, Miguel A, and James M Robins. n.d. “Causal Inference: What If,” 311."
  },
  {
    "objectID": "posts/blog1/blog12.html",
    "href": "posts/blog1/blog12.html",
    "title": "Consequences of exclusion criteria in my cohort data",
    "section": "",
    "text": "People who are be excluded from the cohort I am analyzing are children who had chronic diseases at the time of enrolment into the study as well as families that do no speak English.\nThese chronic conditions included failure to thrive (1-10% of children in high income countries), cerebral palsy (1 to 4 per 1000), cystic fibrosis (1 in 3500), and congenital anomalies (3.5 to 14 in 10,000). Children with asthma and high-functioning autism were allowed to enrol in the study. One question that might be asked, is how will excluding children with chronic conditions impact your results? We had excluded these children because the original aims of the study were about healthy growth. If our target population does not include children with chronic conditions, and we don’t want to make inferences to this population, then we can simply say that doing so is beyond the scope of this study. And there are good reasons to limit it to children without chronic conditions–namely, that children with chronic conditions may have health behaviours and health outcomes (e.g. more severe infection) that are different than otherwise healthy children, and that these children represent a different population that probably deserves their own study.\nIf you think of chronic conditions as a confounder, i.e. children with chronic conditions are more likely to adhere to NPIs, and they are more likely to have maybe more severe manifestations of Covid (i.e. less likely to be asymptomatic), then including children with chronic conditions and not adjusting for them will lead to bias. What this will do is bias the estimate toward the null, because if you can imagine, we will have more exposed infecteds.\nAnother question I get often is how will the exclusion of non-English speaking families impact your results? Cohorts often have this exclusion criterion for two reasons. First, translation of questionnaires (unfortunately) is a very expensive endeavour. Another thing about longitudinal cohort studies is that the long-term relationships that they build with research assistants often keeps families in the study and is key to repeated follow-up. Unfortunately, having research assistants who are able to speak the multitude of languages that are spoken especially in a diverse place such as Toronto is challenging.\nSo by excluding non-English speaking families, we can’t really generalize to this group. We also can’t make inferences to groups that don’t attend primary care, as our cohort is a primary care network. We know that more than 90% of children in Ontario attend primary care within the first few years of their life, which makes primary care an excellent source population for our study, but we still miss individuals. Also, I’m not convinced right now that there is a link between sociodemographic indicators, including non-English speaking, and NPI adherence. What we’re seeing is that we’re seeing highly affluent and educated individuals being non-adherent. And when we don’t have that link, confounding bias by non-English speaking may be negligible.\nHowever, another way to think of these exclusion criteria is through selection bias. Sometimes what happens is that our exclusion criteria means we are conditioning on common effects, or on variables that are descendants of both the exposure and the outcome. Fortunately, chronic illness and language speaking are likely common causes of both the exposure and the outcome (and unlikely to be caused by descendants of the exposure or outcome), and so selection bias is not an issue here."
  },
  {
    "objectID": "posts/blog1/blog9.html",
    "href": "posts/blog1/blog9.html",
    "title": "Assumptions of all the statistical models!",
    "section": "",
    "text": "Let’s talk assumptions!\n\nProject 1: Single Group Interrupted time series:\n(See (Warton?))\n\nPre-intervention trends are linear (can be challenging when there are only a few pre-intervention time points)\nExchangeability between pre and post periods–i.e. there is nothing that changes between our population before and after the interruption.\nAdequate number of measures before and after the intervention\nNo interruptions affecting the outcome occur within the study period\nIntervention is instantantenous.\n\nBenefits of having a control group: the unexposed group will capture temporal changes that would affect the intervention group in the same fashion had the interruption not occurred.\n\n\nProject 2: Parametric g-formula\nSee Hernán and Robins (n.d.)\n\nExchangeability: No confounding\nPositivity: There should be all levels of the exposure in all confounder strata\nAll assumptions required for regression equations in general:\n\nLogistic Regression\n\nIndependent errors\nlinearity (for logistic regression, linearity in the logit for continuous variables)\nStrongly influential outliers\n\nLinear Regression\n\nLinearity\nMultivariate normality\nNo autocorrelation\nHomoscedasticity (error term is the same across all values of the independent variables)\n\n\n\n\n\nProject 3: Cox proportional hazards model\nSee Kleinbaum (2005)\n\nProportional hazards\nLinearity between log hazard and covariate\n\n\n\n\n\n\nReferences\n\nHernán, Miguel A, and James M Robins. n.d. “Causal Inference: What If,” 311.\n\n\nKleinbaum, David G. 2005. Survival Analysis A Self-Learning Text. 2nd ed. 2005. Statistics for Biology and Health. New York, NY: Springer New York. https://doi.org/10.1007/0-387-29150-4."
  },
  {
    "objectID": "posts/blog1/blog15.html",
    "href": "posts/blog1/blog15.html",
    "title": "Defend!",
    "section": "",
    "text": "This is a fun blog where I basically I pick up my proposal and explain how I would defend it, in three brief points. I’ve already gone through my proposal and did the criticizing! Let’s go!\n\nCan you elaborate on the prevalence of neurological and cardiovascular sequelae in your proposal?\n\n\nCognitive deficits ~ cumulative incidence is 5% vs 3%\nSeizures cumulative incidence is 3% vs 1%\nMISC - estimates from the states have this at 2 per 100 000.\nReports of dysrhythmia and increased inflammatory markers.\n\n\nWhat is the impact of your work?\n\nSome of the most sweeping changes to public health in recent memory.\nA record of what children did during this pandemic and an evaluation of the impact\nIf we are the kind of healthcare system that cares about evidence-based interventions and evidence-based messaging, this is something we’ll need going forward for this pandemic and fof future ones.\n\nDo you think that this will change the minds of parents?\n\nThat’s hard to say- We have to respect the value systems and realities of families\nI am the kind of parent who really values science, and evidence–but I also recognize that I have immense privilege that helps me not only to carry out some of the NPIs, but to understand and respect the scientific literature.\nMy job is to carry out the work, provide the evidence, and help families and policymakers carry out informed decision making, not to change their mind.\n\nHow will you deal with a null result?\n\nOne thing to do is to look at the effect size. If we find that the effect is large, and we just don’t have enough precision around our estimate, perhaps it’s an issue with\nAnother thing we’ll have to think carefully is about the comparison we want to make, about the trial we’re looking to emulate. And it’s about a child increasing their adherence by one day a week. Maybe one day isn’t enough. And we’re not asking their mother, father, teacher, classmates, to increase adherence by one day- we’re asking children. maybe that’s not enough either and we have to think about these.\n\nHow will your results apply now that we’re looking at re-infections\n\nYou’re right. Conversation is turning toward re-infection\n\nMy second study is looking at time to first infection; so we’re not really looking at re-infection per-se, using any kind of recurrent event analysis, I think mainly in an effort to simplify an already complex analysis.\n\nBut I think re-infection is an important question, and I think something for a future study, particularly because we know that in the omicron period offers very little protection.\n\nThis project is a start, and I think can certainly lend itself to other kinds of analysis.\n\n\n\nmy third study is not looking at first infection per se, but infection over our study period. This includes children with and without previous covid infection.\n\nHow will you deal with a null result for VE?\n\nFirst we have to deal with our checks and balances. Did we account for all confounders? Are there selection biases at play? Were we able to measure everything accurately. Were our models specified correctly?\nFirst thing to do is look at the point estimate. Is it big? Then look at the precision around that estimate. Perhaps we simply did not have enough children testing with PCR to find enough results\nAnd then, if we think we’ve found a true, null result. This is really important information too. We’ll have to compare to other studies in this age group to see if it’s consistent. Work closely with a knowledge broker.\n\n\n\n\nProject 1\n2. Why a summary measure?\n\nwill be doing each behaviour individually as well\nsummary measure an overall measure of adherence based on 6 behaviours advocated by public health\ntrue that some behaviours are very similar, may weigh more in summary measure; scale development in this area needed\n\n\nHow do you expect those who remain in the study and those who drop out to be different?\n\n\nprocesses the give rise to initial participation in research are often the ones that help retain people\nsocioeconomic indicators: family income, maternal ethnicity, employment, dwelling type, income support, forward sorting area, essential worker status, infection status.\nincluding these factors in the model adjusts for them- so holding these factors equal, what is the expected value of adherence.\n\n\nWhy not weight rather than condition?\n\n\nweighting is also perfectly valid–can do inverse probability of censoring weights\nI suspect results will be similar\nadjustment gives you a result that is straightforward, intuitive but we can certainly consider weights\n\n5.How did you come up with this list of baseline confounders? - parents of the exposure, parents of the outcome, and non-instrument - literature - DAG\n\nWhy did you decide to do an interrupted time series analysis?\n\n\nlent itself well to the question, figuring out the jump and drop in adherence, and the trends over time\nalso good since we have multiple time points before and after the drop\nit’s relatively well easy to interpret\n\n\nWhy not a difference in differences analysis?\n\n\nrequire a control group that did not experience closures at same time\ncontrol group needs to be similar socially, demographically, politically - I’m not sure I have a similar one\nI don’t have NPI measures anywhere else\n\n\nHow will you deal with the cadence changes?\n\n\ntry to treat it as continuous as much as possible.\nthere is going to be interpolation between measures, but cadence changes where necessary to keep engagement up in the cohort and avoid attrition.\nin the event where we find we just don’t have enough time points to assess jump/drop adherence before and after the interruption, we’ll consider a pre-post analysis, which is not as strong and doesn’t enable you to see slopes or immediate jumps for that matter. We consider synthetic controls–for example, there’s a cohort out in Montreal that may or may not use similar measures to our own, but as to the availability of the data, we’re not certain.\n\n\nWhy random intercepts and not random slopes?\n\nI think we could consider random slopes too–if we think there’s going to be a large variation in NPI adherence over time within individual and within family.\nPlan is to start simple and build complexity\nUse likelihood ratio tests to see if adding random slopes contributes significantly to the fit of the model\n\nWhy does your SS calculation consider 0.5 days difference a week an important change?\n\nHalf a day seemed a good place to start.\nConsidered going down as low as 0.25 days, means you’ll need about 1300 participants in the study.\n\nCan you consider splines in segmented linear mixed effects regression?\n\nGreat question. If I want to be able to compare slopes before and after the jump, this will be hard to do in a straightforward way with splines.\nWe’ll have to choose a timepoint before and after the jump to compare.\n\nWhy did you consider such large loss to follow-up?\n\nGreat question. We know that the cohort study had really excellent follow-up after 6 months, as high as 80%.\nWe know that this has declined since then- the extent to which it has declined, I haven’t yet seen the data yet, so I can’t comment here.\nWe’ll try to mitigate this by:\n\nadjusting for factors associated with censoring\nwe’ll use multiple imputation to impute covariates\nreally consider the factors that are on that backdoor path opened by only including the non-censored, and adjusting where we can to block that path.\nConsider inverse probability of censoring weighting/ standardization methods\n\n\nWhat will be the impact of large loss to follow-up on your results?\n\nIf there is substantial drop-out, we are going to have a consideral selection bias probelm to worry about. And the direction and size of that bias is hard to say at this stage.\nand we’ll have to use every tool at our disposal to help it\n\nInverse probability of censoring weighting, standardization by censoring\n\nMultiple imputation,\nExamining the backdoor paths\n\n\n\nWhat chronic conditions are excluded from your cohort?\n\nTARGet Kids! is about Healthy growth\n\nSevere congenital abnormalities\nPrader-Willi syndrome\nFailure to Thrive\n\n\n\n\n\n\nProject 2\n\nHow did you choose the variables?\n\nFigure 2 is not the entire list, this is for demonstrative purposes\nAppendix has a big list of variables that were chosen by confounding criteria according to Vanderwheele and Shpitser-includes things like sociodemographics, parent occupation and financial security, family history of illness and pregnancy, childcare arrangement, public health preventive measures, child health condition, parent health condition, infection status, etc.\nNow, this is a big list. And there might be limits to what our models can accommodate, so we may have to choose the variables that are most proximal to our outcome\nThe only way to do this is via DAG, so we’ll have to draw an extensive DAG.\n\nAre you worried you are putting too many variables in the model?\n\nYes, there are some guidelines on how many variables we can put in a model per number of outcomes. The latest by Harrell is 15 events per outcome. If we think about infection rate at about 13% (which is low), that will give us about 170-200 outcomes, which means about 10-13 covariates.\nThis is a rule of thumb and we will have to play this by ear depending on how well the model converges, fit statistics, etc.\n\nWhat kind of fit statistics?\n\nFor our linear models, plots of residuals to ensure normality across each of covariates, and heteroscedasticity–plotting residuals across observations. ROC curves. Measures of sensitivity and specificity.\n\nHow will changing RAT availability change your results? - What this means is that the degree of misclassification of the outcome will potentially over time.\n\n-   Misclassification will depend on degree of availability of RATS, and the use of those rapid antigen tests.\n\n-   The direction and degree to which misclassification will impact results will depend on other covariates in the data (it is differential!)\n\n-   Luckily, we do have quarterly measures of serology, which means we have bias parameters that will change over time--so recognize that the degree to which this misclassification bias impacts our results is going to change over time. and that's key.\n\nHow do we account for contextual changes?\n\nContextual changes act as really important confounders\nimportant to adjust for them or\nthink of the downstream factors that we think can be proximal confounders in order to block those backdoor paths.\n\nHow do we account for clustering in parametric g-formula?\n\nClustering is an issue having to do with the standard errors\nStandard errors will look falsely narrow\nThere are cluster bootstrap methods that I am exploring.\n\nRecruitment is open longitudinal.\nwill you find a different in risk?\n\nWe will find a hazards ratio\nThis is a relative difference in risk in the counterfactual\n\nSo not an absolute difference, but a relative difference\n\n\nWhy not use something like the e value\n\nThe e value is the minimum association with Exposure and the outcome that would be needed to explain away the effect\nReceived criticism\n\nEven if you have a high e value, it is possible that cumulative unmeasured confounding adds up to be more than the value of the e value\nalso, doesn’t take into account correlations between unmeasured confounders and other variables,\n\n\nWhy not use marginal structural models for time-varying exposures?\n\nWeights can become very unstable with marginal structural models\nbecause need to multiply weights at every time point; this means weights become very big.\nAlso, from my understanding parametric g-formula has the edge when it comes to positivity violations–so the idea that we might not be able to see each level of confounder in all levels of covariate strata.\n\nWhat are some other examples where the parametric g-formula has been applied?\n\nMechanical ventilation in the ICU and mortality.\nair quality and mortality\nbone marrow transplant and mortality\n\n\n\nProject 3.\n\nWhy not use propensity matching rather than test negative?\n\nimportant to consider other ways of doing this study\nPropensity matching is more challenging to do when you’ve got a time-dependent exposure, such as vaccination. Propensity scores are typically generated for each individual at study entry. But children who vaccinated very early in the study are different than those who vaccinated toward the end.\n\nSo you need to conduct time-dependent propensity score matching, where at each time point, you find a propensity score match between a vaccinated individual and an unvaccinated individual.\n\nI think there are some efficiency things you might have to consider here–you might not find a suitable match for every vaccinated individual at every time point, even at large sample sizes.\nCan you walk us through some of the selection bias test-negative studies can present?\n\n1.  Yes. The first has to do with external generalizability. We are only looking at children who test. They are very different from children who do not access testing, for a myriad of reasons, including income, resources, time, distance to clinic, etc.\n\n2.  \n\n3.  \n\n4.  Type 2 selection bias--\\> effect modification of selection for the relationship between exposure E and D.\n\nTell me about the datatsets in ICES and how you will use them\n\n\nDIN - Drugs list. To figure out if they had flu vaccine; to figure out if they are on immunosuppressive medication\nPCCF - Postal code conversion file. For neighbourhood income quintile.\nCORR - Canadian Organ Replacement Registry. For exclusion criteria- solid organ transplant\nDAD - Discharge Abstract Database. For exclusion criteria- immunocomp.\nNACRS- National Ambulatory Care Reporting System. For exclusion criteria - immunocomp.\nODB - For flu shot,\nOHIP - for linking datasets\nSDS - Same day surgery. For immune disorder flag.\nASTHMA - Ontario Asthma Dataset.\nHIV - Ontario HIV Dataset. For immune disorder flag.\nOCCC - Ontario Crohn’s and COlitis Cohort dataset. FOr immune disorder flag.\nODD - Ontario Diabetes Dataset. For comorbidities\nCENSUS - For area level ethnicity data\nRPDB - Registered Persons Database. For figuring out age, sex, alive/ dead.\nOLISC19 - OLIS Covid-19 Laboratory data\nONMARG - Area level co\nC19INTGR - This data set is a comprehensive set of all available COVID-19 diagnostic laboratory results in Ontario, derived from 3 data sources: the OLIS; CCM; distributed testing data from laboratories within the COVID-19 Provincial Diagnostic Network.\nCOVaxON - Date, prodcut type, dosage,\nCCM s45 - Information on COVID-19 cases such as symptoms, epidemiological contacts and risk factors are not complete in the OLIS data nor are captured in other health administrative databases. Thus, to enable more descriptive reporting on COVID-19 cases, ICES started receiving daily feeds of the Public Health Case and Contact Management (CCM) Solutions database, the centralized database containing all confirmed COVID-19 cases in Ontario, which was an entirely new data holding for ICES.\n\n\nWhat comorbidities will you be adjusting for?\n\nAsthma\ndiabetes\nimmunocompromising conditions due to underlying diseases or therapy\nautoimmune diseases\nactive cancer\npediatric complex chronic conditions·\n\nWhy not use population controls?\n\nWritten about this by Vandenbroucke and Pearce\nSo they were saying because in a pure TND, we’re comparing those who get sick from covid with those who get sick for some other reason, the factors that impact these outcomes may be very similar.\nSo a way around this is by using population controls\n\nVandenbroucke suggest using the accompanying person as a control–this might work for adults,\n\nThese are people who would have accessed testing if they were ill but did not.\n\nbut not really for children and hard to collect in ICES data\n\n\nOther population controls possible\n\nBut you really want to make sure that these people represent children who would have accessed testing if they were ill- so the underlying source population should be the same.\n\n\n\n\n\nCol1\nCol2\n\n\n\n\nCCM\n\n\n\nDIN\nDrugs List\n\n\nDAD/NACRS\nThe National Ambulatory Care Reporting System\n\n\n\n         (NACRS) captures all visits to hospital EDs beginning in  \n         2002. As with the DAD, each row of the NACRS contains     \n         demographic, diagnostic, procedural and treatment         \n         information for each emergency room visit.35              |"
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Test Negative Studies: Let’s Talk About It",
    "section": "",
    "text": "If you’ve been keeping up with the Covid-19 vaccine literature, there may be a study design that sticks out to you: a test-negative study. Huh? What’s that?\nBefore we get into what a test negative study is, let’s think about what these vaccine effectiveness studies want to examine. The exposure in these studies is typically vaccination status (0,1,2 doses, typically with 0 as the referent category), and the outcome is SARS-CoV-2 infection. The question is, what is the real-world impact of vaccines on SARS-CoV-2 infection?\nBut back up–why do we need to know real-world effectiveness of vaccines? Don’t we have RCT data on how well the vaccines work? Yes, we do- but these RCTs typically have very stringent inclusion criteria (children have to be very healthy, etc.), and the families that self-select into these studies tend to be very affluent, well-connected, educated. Moreover, RCTs tend to have very regimented follow-up and dosing—families are reminded that they need to come in for their second dose, an RCT coordinator makes sure they receive it, vaccine administration is highly protocol-ized (vaccine expiration and storage are well-monitored). In the real world, it doesn’t always happen this way. You’ve got kids who are maybe not the healthiest–they might have chronic conditions like asthma. Families aren’t always rich and educated. Families may not come for their second dose, or may not come on time. Vaccine distribution is challenging, with changes in expiry and variable storage abilities. If we want to make inference to a population that deals with these system-level challenges, that has some health conditions, that are not as rich/well connected, then we need to do real-world effectiveness studies.\nOk, so how do we do this? In an ideal world, we could conduct a prospective cohort study—recruit children, follow them up over time, surveil them daily to see who got infected with SARS-CoV-2 and when. Sounds simple right? The problem is that this is really, really resource intensive. You need to follow a lot of children to have precise estimates. You need to test these children every day, and even if you tested them weekly, this can mean thousands upon thousands of PCR tests. You might have hard time finding families/children who are willing to do this.\nOk. So cohort studies are too expensive. What the next best thing? The next best thing might be a population-based case control study. Here we would have complete case ascertainment, and we would select a random sample of population controls. This study, in essence, could be viewed as nested within the cohort, with missing data (i.e. controls we did not select) as missing at random. In other words, according to Sullivan, this would be a cohort study in which the statum of people who do not test are ignored. This study would be much cheaper, as we wouldn’t have to do hundreds of thousands of PCR tests. But population controls are challenging—the way that they are ascertained might be different from the way cases as ascertained. The population controls might be very different than the cases. In our scenario, cases are ascertained through testing centres or pharmacies or other places that do PCR tests. These people might have better access to healthcare than the general population. This means the cases and conrols will be non-exchangeable–there are confounding factors that will impact case ascertainment as well as vaccination status, meaning there will be bias.\nOK. What about a case-control study with ‘other’ patient controls–i.e. people who can access the testing centres and pharmacies. Now we’re onto something. These controls are more like our cases—there is some indirect matching going on here. This brings us to the test-negative study. A test-negative study recruit participants who attend Covid assessment centres and test positive for Covid-19; controls are participants who undergo the same tests for the same reasons as the cases, but test negative.\nThe advantage of test-negative studies is that they have similar participation rates between cases and controls, similar information quality and completeness, and similar access. A particular advantage of test-negative studies is that they compare individuals with similar health seeking-behaviour and access to testing. We might imagine that a person who is more likely to seek vaccines is probably also more likely to find a way to get a PCR test. This is especially important given testing restrictions, which excluded many individuals from obtaining a test after December 2021. Residual confounding might also be less of a concern in test negative studies because (see Shi et al) variables that impact vaccination status are similar for Covid-19 and non-Covid-19 respiratory infections (e.g. children with parents who are healthcare workers might be more likely to get the vaccine, but the proportion of children with healthcare workers who have Covid-19 versus some other respiratory infection is probably the same).\nSome people have had been critical of test-negative designs. Westreich et al. state that test-negative designs are not really case-control studies as controls are not formally sampled from the source population. However, as Vandenbroucke and Pearce point out, there is a broad class of case control studies that use ‘other’ patient controls, which can sometimes help with exchangeability between the cases and controls. These designs using ‘other’ patient controls can provide valid estimates when (according to Jackson and Nelson) a) other respiratory viruses are not impacted by the vaccine and b) vaccine effectiveness does not vary by health-care seeking behaviour.\nOthers have asserted that a test-negative study is a cohort study with some incomplete follow-up. However, there is no follow-up happening in test negative designs because case and control status are ascertained at a single point in time: the time of the test (Vandenbrocke and Pearce).\nHowever, under the assumption of no confounding, selection bias, misclassification and assuming parametric assumptions are met, the test-negative study can provide valid estimates of vaccine effectiveness."
  },
  {
    "objectID": "posts/blog1/blog14.html",
    "href": "posts/blog1/blog14.html",
    "title": "Controlled Direct Effects- What is going on?",
    "section": "",
    "text": "In a typical IPW estimate, you find weights for the exposure in order to ensure balance across the exposure. You then insert the weights into a model for the outcome with the exposure as the covariate (you can also insert other covariates here for a ‘doubly robust’ approach). The beta coefficient here will get you the marginal estimate of the effect of the exposure on the outcome\nWhen we’re thinking about controlled direct effects, the effect of an exposure on an outcome if we could set the mediator to a particular value, then we find weights for the exposure and the mediator in order to ensure no unmeasured confounding between the outcome and the exposure and the mediator. Now, we insert the weights into a model for the outcome with exposure, mediator, and an interaction between them. Now can we interpret the values of the coefficents in the CDE model? No, we can’t! Because including the value of the mediator explains away some of the effect of the exposure.\nInstead what we need to do is standardize the estimates–find the predicted value of each of the scenarios (Exposure ON, mediator set to 1; Exposure OFF, mediator set to 1), and then average over them! These will give us standardized estimate (e.g. risk) of the exposure-outcome relationship, without worrying about about the mediator in the model!"
  },
  {
    "objectID": "posts/blog1/blog8.html",
    "href": "posts/blog1/blog8.html",
    "title": "Please, what is the parametric g-formula?",
    "section": "",
    "text": "The parametric g-formula has terrible branding. The name sounds like some kind of rocket booster requiring advanced-level physics to operate it.\nAnd although it may sounds complicated, it really comes to thinking about it as being akin to standardization. In epi 101, we learn that standardization really means finding the weighted averages that make two groups comparable (if we think about clinical trials, the whole point of randomizing people into two groups is to make them more comparable- this is called exchangeability). For example, say we want to see if living in country A causes cancer, compared to living in country B. But say we know that age and sex are associated with cancer (e.g. older individuals and women are more likely to have this kind of cancer), and country A just happens to have a high proportion of women and older individuals. What we want to do is weight the groups in a way that they have similar distributions of women and older people, so that they are more comparable. The general formula for a standardized estimate of Y is\n\n\n\nwhere \\(A\\) is the level of exposure, \\(C\\) is censoring, and \\(L\\) are the level of covariates.\n\n\nWhen there aren’t many variables, we can get this non-parametrically (i.e. we don’t need to use models!). Basically, what we need to do is find the mean value of Y in each of the confounder strata (that’s the conditional means, the \\(E[Y|A=a,C=0,L=l]\\) part), and weight it (multiply it) by the probability of being in that stratum (that’s the \\(Pr[L=l]\\) part. So, in our mortality example, let’s say we only had two age groups: young and old. We would find the mean value of \\(Y\\) in young women and multiply it by the probability of being a young woman, the mean value of \\(Y\\) in old women and multiply it by the probability of being an old woman, the mean value of \\(Y\\) in young men and multiply it by the probability of being a young man, and the mean value of \\(Y\\) in old men and multiply it by the probability of being a young man. Then we would add up all these terms, and voila, there’s your standardized risk of mortality for that specific country.\nNow say we have a ton of variables- sex, age, education, income, comorbidities, children, marital status, etc. It’s going to be really hard to do this non-parametrically because there’s simply not enough data to fill the thousands of strata created by every combination of confounders.\nWe need to resort to modelling. What we can do is develop a regression, conditioning on the confounders! After all, what a regression is really trying to do is model the $E[Y|A=a, C=0, L=l]$. Fantastic! Then what you do is using your data, you set \\(A\\) to equal the value of country A. You find the predictions.\nNow, you say-should we weight by the probability of being in that stratum? Again, it’s hard to find this when not every stratum is filled (it’s asking your data- what’s the probability of being an old woman, with this amoung of education, this income, this particular comorbidity, 6 children, unmarried, etc). Fortunately, there’s a fun trick- the weighted mean can be written as the double expectation. That is,\n\\(\\sum_l E[Y|A=a, C=0, L=l] \\times Pr(L=l)= E[E[Y|A=a, C=0, L=l] \\times Pr(L=l)]\\)\nWhat! What does this mean- it means we can find the mean of the conditional means and this ends up being your standardized mean! What! Amazing. So in summary, there are three steps:\n\nModel the conditional mean\nPredict the value of the conditional means for all of your confounder strata combinations.\nFind the mean of the conditional means in each group; find the risk difference, risk ratio, etc.\n\nThe parametric g-formula for time varying-exposures takes it a step further by modelling both the outcome and each of the time-varying covariates at each time point, and then applying simulations. Why might we need simulations? Well, if we were solely to rely on our dataset for step 2, we might not have people who fulfill all of those combinations of confounder strata; so we wouldn’t be able to estimate the sum of the conditional means.\nHow do you do a simulation?\nYou use something called Monte Carlo simulations–so based on your models from Step 1, you get predicted probabilities of Y, based on your covariates. Then you flip biased coins to determine which value a person will get.\nThis is where it gets interesting. THe first simulated dataset you’ll make is basically like your original dataset, except with far more people. But nothing’s been done, you haven’t altered your exposure in any way. This is the ‘natural course’ dataset. The second dataset however, whenever you simulate your time varying exposure, whatever value you get, you’ll increase that value by ONE DAY. This is the ‘increased by 1 day dataset’.\nThen, you predict the value of your conditional means using your simulated dataset–this time, you should have enough people in every combination of your confounder strata.\nThen, you find the mean of the conditional means for the ‘natural course’ dataset. And you find the mean of the conditional means for the ‘increased by one day’ dataset. If you subtract these, you will have the marginal mean difference! Wow!\nFurther recommended reading is available in Hernán and Robins (n.d.) and Keil et al. (2014).\n\n\n\n\nReferences\n\nHernán, Miguel A, and James M Robins. n.d. “Causal Inference: What If,” 311.\n\n\nKeil, Alexander P., Jessie K. Edwards, David R. Richardson, Ashley I. Naimi, and Stephen R. Cole. 2014. “The Parametric g-Formula for Time-to-Event Data: Towards Intuition with a Worked Example.” Epidemiology (Cambridge, Mass.) 25 (6): 889–97. https://doi.org/10.1097/EDE.0000000000000160."
  },
  {
    "objectID": "posts/blog1/blog5.html",
    "href": "posts/blog1/blog5.html",
    "title": "What is record-level quantitative bias analysis?",
    "section": "",
    "text": "In my proposal, there are a number of times when I mention that I am going to be doing record-level quantitative bias analysis. Though I am explicit about what the required bias parameters are, what will I do with those bias parameters after I’ve found them?\nIn the Fox and Lash book (2020), they list a couple of steps needed to do record level QBA. These are:\n\nAssign probability distributions to each of the bias parameters\nUse simple bias methods to generate bias-adjusted data to inform the bias analysis and apply bias parameters probabilistically\nSave bias adjusted estimate and repeat steps 4a to c\nSummarize bias adjusted estimates with a frequency distribution that yields a central tendancy and simulation interval\n\nLet’s go through them in detail.\n\n1. Assign probability distributions to each of the bias parameters\nOnce we have established the bias parameters for your analysis (in the case of outcome misclassification, it is SE, SP, PPV, NPV), first, it’s important to remember that there’s error around each of your bias parameters. Even though you found a single value from, for example, the literature, there is a range of plausible values for each bias parameter. We can assign a distribution of probabilities around the estimated parameter (e.g. trapezoidal, normal distribution, uniform distribution)\n\n\n2. Use simple bias analysis methods to incorporate uncertainty in the bias parameters and random error\n\n2a. Obtain using the bias parameters, calculate misclassification-adjusted sensitivity, specificity, PPV, NPV of the outcome for every record in the dataset.\n\nCreate a simple contingency table of your exposure-outcome relationship.\nUse Monte Carlo sampling techniques to select bias parameter values from the probability distribution in 1.\nUse the sampled bias parameter to correct the cells in the 2x2 exposure-outcome contingency table. Insert the values of the corrected cells as new variables for the first record.\nFrom these four variables, calculate sensitivity, specificity, NPV, PPV. Insert these values as new variables for the first record.\nRepeat ii) to iv) again for all the records in your dataset.\n\n\n\n2b. Reclassify the outcome, use reclassified outcome to obtain new estimate of association, and incorporate random error\n\nUse Bernoulli trials to use the values calculated in A to determine if the record had the outcome or was censored–the ‘reclassified’ outcome status.\nUse the reclassified outcome status in your new regression analyses.\nSimulate random error and incorporate it into your estimate.\n\n\n\n\n3. Save this estimate and repeat all steps above for X number of iterations.\n\n\n4. Summarize the distribution of bias-adjusted estimates using a simulation interval."
  },
  {
    "objectID": "posts/blog1/blog4.html",
    "href": "posts/blog1/blog4.html",
    "title": "Machine learning - opportunities?",
    "section": "",
    "text": "OK, so for one of my projects, I’m using some pretty complicated analyses–in particular, something called the parametric g-formula, which sounds a little crazy and spaceship-y. When you boil it down, all it’s doing is standardizing, which essentially means finding weighted averages across all of the counfounder strata.\nAlthough I wouldn’t exactly call it machine learning (what is machine learning, really, anyway?), the parametric g-formula is considered ‘advanced analytics’ because it requires quite a bit of computing power. In order to calculate those weighted averages, you need to need to run simulations so that you can get representation in each of those millions of combinations of confounder strata. And you simulate the covariate history, you simulate the censoring or outcome, for each of the many weekly timepoints in my analysis. This requires a ton of models. And, as we know, models each come with many assumptions- linearity, normality of the residuals, that the variance of the residuals is constant across the the observations (homoscedasticity), that there are no unduly influential points. When there are hundreds of models, model misspecification can produce a sizeable bias.\nWith the advent of machine learning methods, some of these assumptions might be relaxed. Machine learning can predict the probability of each variable at each time-step (see Blakely et al., 2020), potentially being made ’more robust to model misspecification through machine-learning techniques” (Westreich et al, 2015). These might include tree-based methods, random forests, and neural networks, for example.\nIt’s interesting because I think a lot of the field has been sort of bemoaning the fact that machine learning applications are restricted to prediction-type research questions–can we predict, for example, who will get covid, based on this set of covariates?\nBut even in causal inference methods, prediction can be part of the process. In the parametric g-formula, we actually need to predict the covariate history and outcome for a large, simulated dataset. We need not restrict ourselves to parametric models, though computationally, we may have increased challenges. I suspect in the future, with even greater computing power, we will see more machine learning methods being applied in the causal inference space (as we already have!) This is an area of growth in the field of epidemiology."
  },
  {
    "objectID": "posts/blog1/blog6.html",
    "href": "posts/blog1/blog6.html",
    "title": "Decomposition and probabilities",
    "section": "",
    "text": "Now to move onto something a bit different. Decomposition! Decomposition is the process of separting effects into an indirect and direct effects for mediation analysis.\nLong story short- but we want to obtain the ‘nested counterfactuals’–ie. what the expected value would be if the mediator had acted in a way differnt than the actual treatment received.\nLet’s start with direct-indirect decomposition.\nThe formula for this is TE = NIE + TDE\nWhere \\(TE=E[Y^{(1,M^1)}]-E[Y^{(0,M^0)}]\\) (both switches on minus both switches off)\nwhich, if we add and subtract this term, called the nested counterfactual: \\(E[Y^{(1,M^0)}]\\)\nturns into: \\(TE=E[Y^{(1,M^1)}]-E[Y^{(1,M^0)}] + E[Y^{(1,M^0)}]-E[Y^{(0,M^0)}]\\)\nGreat! Now how do you get each of these terms? Remember, each term is made up of an exposure set to a value, with a mediator taking on one of two values, 0 or 1 (because ‘M under A=1’ can be either 0 or 1). We need to take a weighted average of each of those scenarios.\nThe first and last terms are easy:\n\\(E[Y^{(1,M^1)}]=E[Y|A=1,M=1]*Pr(M=1|A=1) + E[Y|A=1,M=0]*Pr(M=0|A=1)\\)\nBasically, it’s expected value of Y when A is set to 1 and M is also 1 weighted by the probability that M would take the value of 1 when A is set to 1 PLUS the expected value of Y when A is set to 1 and M is 0, weighted by the probability that M would take the value of 0 when A is set to 1.\nThe nested counterfactual is tricky:\n\\(E[Y^{(1,M^0)}]=E[Y|A=1,M=1]*Pr(M=1|A=0)+E[Y|A=1,M=0]*Pr(M=0|A=0)\\)\nHere, A remains fixed at 1, but M acting as if A were 0 can take on two values, 0 or 1. The term \\(E[Y^{(1,M^0)}]\\) means we need to add up the probabilities of those two scenarios. Before, it was easy- we basically took a weighted average of the two possible values of the mediator. The weights were easy because it was the observed probability– the probability of M conditional on the exposure level we actually saw.\nThis time, which probability do we take? In the previous example, we were interested in M ‘as if A were equal to 1’. Well guess what? A is equal to 1, so we simply weight by the probabilities that we see M=1 or M=0 in the category of the actual exposure value, A=1. This time, remember we are interested in M ‘as if A were equal to 0’. Well, this time, our actual exposure value A is equal to 1, how can we see M ‘as if A were equal to 0’? No such group really exists. This is why we need to look to the other group, A=0, and see the probability that M would take on 1 or 0 in this group. and multiply this by the expected value of the a scenario we can see."
  },
  {
    "objectID": "posts/blog1/blog7.html",
    "href": "posts/blog1/blog7.html",
    "title": "Why not use a cohort study instead of a test-negative study? (and vice versa)",
    "section": "",
    "text": "When we see vaccine effectiveness studies, we often see two types: test-negative studies and cohort studies.\nCohort studies are really nice because they enable us to obtain the relative risk, which is arguably more readily interpretable than the odds ratio. They can also establish the prevalence of a disease in a population.\nTest-negative studies have the advantage of being relatively cheaper and more readily conducted. This is not really true when we talk about population health administrative data, where the data is collected prospectively no matter what.\nSo why would we choose to do a test-negative study instead of a cohort study, or a case-control study for that matter?\nThere are two mains reasons(Jackson and Nelson 2013) are misclassification bias and confounding.\nIn a perfect world, we could fully capture vaccination, infection, and healthcare seeking behaviour in our population, we could make this 2x2 table:\n\n\n\nFrom Jackson and Nelson (2013)\n\n\nAnd the estimand here is (in the absence of effect modification by care-seeking behaviour):\n\nThe only way we know if someone got Covid-19 is if they got a PCR test. So our outcome is ‘tested positive for Covid-19’. When we do a cohort study, our ‘did not test positive for Covid-19’ group is sampled from B, C, D, E, F for the vaccinated and the ‘did not test positive for Covid-19’ group is sampled from H, I, J, K and L for the unvaccinated group. Similarly, when we do a case-control study, we sample controls from B,C,D,E,F and H,I,J,K,L.\nThis will lead to biased results from misclassification because group D and group J, the infected with influenza but did not seek PCR testing, will be misclassified as ‘not infected’ in the cohort study, and as controls in the case-control study. This will likely bias the results toward the null, as those who do not vaccinate are also less likely to test, and more likely to appear ‘not infected’.\nThere is also bias from healthcare-seeking behaviour because those with a higher healthcare seeking behaviour propensity will be more likely to vaccinate and also more likely to seek testing.\n\nHow does a test-negative study avoid misclassification bias and confounding bias?\nBy restricting on those who test, one no longer needs to sample from the ‘not care seeking’ population, so misclassification that exists from including those who are actually positive but did not test, is no longer an issue.\nAlso, by restricting on those who test, we block the backdoor path that can give rise to biased estimates.\n\nBest of all, we can still get valid estimates of VE (see below), though our ability to generalize to the not tested is limited.\n\n\n\nBut wait! Doesn’t your other study use a cohort design?\nWhy yes it does! Thanks for asking. It would be challenging to do a test-negative study with our cohort data because we would have a limited sample size to find an effect.\nWe are trying to mitigate misclassification by including tests reported by parents via rapid antigen tests. We also have an extensive quantitative bias analysis planned for misclassification outcome by using record-level probabilistic QBA processes.\nWe will try to mitigate confounding by health-seeking behaviour by adjusting for visits to the family doctor, and other indicators of health-seeking behaviour including education, income, ethnicity, and attitudes toward vaccines.\n\n\n\n\n\nReferences\n\nJackson, Michael L., and Jennifer C. Nelson. 2013. “The Test-Negative Design for Estimating Influenza Vaccine Effectiveness.” Vaccine 31 (17): 2165–68."
  },
  {
    "objectID": "posts/blog1/blog3.html",
    "href": "posts/blog1/blog3.html",
    "title": "Limitations of the data",
    "section": "",
    "text": "A question I get often when I’m doing a talk is, “what are the limitations of the data that you’re using?” And it’s a great question. We should always think what our data can tell us and what it cannot.\nOne of my first data sources is a large children’s cohort study. The strengths of this data is that it has rich, repeated measures about adherence to NPIs, about parent-reported infection, symptoms, sociodemographics, lifestyle, child health behaviours. It samples from primary care, which is great because we know that more than 95% of children access primary care in the first few years of life.\nHowever, it also comes with another limitations. I will frame these limitations in terms of the three types of bias: selection bias, measurement error, and confounding.\nSelection bias: Like many other cohorts, it suffers from selection processes that make it easier for richer, more educated, whiter participants to participate in the study. This may limit its external generalizability. Another limitation, in the some vein, is that richer, more educated families tend to remain in the study longer than those who are not. What does that mean? Well, if richer, whiter people are more likely to adhere to masks, and they are also less likely to get infected, and it would bias the resut away from the null.\nMeasurement error: Like other studies that use survey data, there is a certain amount of measurement error that comes along with questionnaires. For example, families might have trouble recalling how many days their child was adherent to masks. There may be some measurement error associated with self-report of rapid antigen tests, which in themselves have imperfect diagnostic accuracy, which then must get reported correctly by parents. Moreover, the tests must be administered properly, which may not occur.\nConfounding: One of the strengths of cohort studies is that they can collect really rich confounder information. For example, TARGet Kids! has great information not only on child NPI adherence, but they also have information on parent NPI adherence, household income, maternal and paternal education. However, there are measures that we have not collected–for example, as much as we can try to control for ‘health seeking behaviour’ through other measures (visits to the doctor, flu shot in past few months, perception about health), there may still may be some residual confounding.\nThe next data source are population health administrative data. This is data that is usually collected for billing and administrative purposes. Strengths of administrative data include the fact that it is population-wide–in our data, we can capture every single community dwelling child that obtained a vaccine in the province. This means we have a lot of data–a lot more than if we were to look at, for example, cohort data–which enables us to get more precise estimates. Administrative is also fast and efficient- data is recorded in real time, enabling us to get answers quickly. And lastly, administrative data is extremely resource-efficient; we do not require RAs, infrastructure to collect this data, as it is already being collected for administrative and financial purposes.\nBecause this data is not being collected for research purposes, it also comes with a number of drawbacks:\nSelection biases: Selection biases still exist will health administrative data (even though they do capture every encounter with the healthcare system), because they require participants to have access to the healthcare system. Unequal access is common, and in our case, it is exacerbated by government restriction on testing at the end of December 2021. In the third study, we are restricted to participants who were able to access testing for their child. For the most part, this included children of essential workers working (including physicians). However, although government restirictions were the same across Ontario, the manner in which they were implemented from hospital to hospital differed. This may mean that the external generalizability of our results to the general Ontario population may be limited.\nMeasurement error: Health administrative data can suffer from undercoding or miscoding of variables. It is unlikely for vaccinations to be miscoded, given the stringency of the vaccination protocol and the tracking of every single dose. PCR testing does not have perfect sensitivity and specificity, though it is considered the gold standard for testing for SARS-CoV-2.\nConfounding: Because this data is not collected for research purposes, there are a number of variables that may be important confounders for which we lack information. For example, many individual-level sociodemographics are not available–income, ethnicity, education. For our VE study, we use area-level sociodemographics as a proxy. These include neighbourhood income quintile, essential worker status quintile, and large household size quintile. These may or may not serve as good proxies for individual-level characteristics.\nThese data sources come with their limitations. However, they also come with a number of complementary strengths- the cohort has information that ICES lacks, and ICES has information that the cohort lacks, and it also covers many many children. It is important to leverage these strengths to make the most out of available data to answer important questions about SARS-CoV-2 in children."
  },
  {
    "objectID": "posts/blog1/blog2.html",
    "href": "posts/blog1/blog2.html",
    "title": "Who are your controls in test-negative studies?",
    "section": "",
    "text": "One thing that comes up in test-negative studies is, who exactly are the controls? Repeat after me: your controls are participants in your study who tested negative. If you’ve been working with cohort data or RCT data a lot, this is not intuitive: we often think in terms of exposed and unexposed. Controls are not your unexposed; they are those who tested negative.\nThe overall test effectiveness study will just compare those who test positive with those who test negative, full stop. [Note: you’ll still have to adjust for calendar date, because what might happen is that perhaps as the pandemic rolls on, as we change seasons, people are more likely to get vaccinated and also perhaps more likely to test positive. This biases the result toward the null]\nWe can do interval analysis too-e.g., what is the vaccine effectiveness, or impact of vaccines, on children who were vaccinated only 14-29 days prior to their index date (remember that the index date is the day that they take their test). Another way of phrasing it is, what is the vaccine effectiveness 14-29 days after vaccination?\nIn interval analysis, who are the controls? Again, they are still children who tested negative in that interval. How to know if they are in the interval? For vaccinated test-negatives (controls), you’ll have an ‘interval date’, which tells you how along ago vaccination happened. In our example, you’ll select vaccinated controls who vaccinated 14-29 days before their index date. For unvaccinated control, this interval date doesn’t really make any sense. From which date should we count the unvaccinated control (or case for that matter)?\nRemember our time zero is July 28, 2022 (when Ontario rolled out vaccines for children in this age group). We can calculate a ‘time interval’ for unvaccinated children between their index date and this date.\nTake for example, the stratified/interval analysis in children who tested 14-29 days ago. Let’s make a 2x2 table below.\n\n\n\n\n\n\n\n\n\nTest negative\nTest positive\n\n\n\n\nUnvaccinated\nUnvaccinated 14-29 days ago\nUnvaccinated 14-29 days ago.\n\n\nVaccinated\nVaccinated 14-29 days ago\nVaccinated 14-29 days ago\n\n\n\nWe should be looking at everyone 14-29 days prior to their index date, or 14-29 days ‘after vaccination’- but for the unvaccinated, ‘after vaccination’ doesn’t make sense because they were never vaccinated. But we can look at the interval between the roll-out date and their index date. As long as it was greater than 14-29 days, they can be considered. An index date that happened 90 days after roll-out for the unvaccinated can be included in the comparison of 14-29 days, because unvaccinated at 90 days also means unvaccinated at 60 days, and unvaccinated at 30 days. That means we can include everyone who is unvaccinated at an index date greater than the interval we are interested in, in our analysis, whether they are test negative or test positive.\nOther notes:\nLet’s talk about ‘follow-up,’ which is a bit of a tricky concept. Note that we can’t really talk about ‘incomplete’ or ‘complete’ follow-up in this analysis because outcome ascertainment happens at one point in time: the time of the test (see last paragraph in this blog post). But there is kind of a follow-up time for the vaccinated group, the time from vaccination to the index date; in the unvaccinated group, the ‘follow-up’ time is a bit arbitrary—it’s counted from time zero, date of vaccine rollout in Ontario. But remember, a person who is unvaccinated at their index date, which happens, for example, 90 days after time zero, is also unvaccinated at time 60, time 30.\nWhat about someone who with multiple tests–they tested negative at 1, then tested positive at another, and they changed their vaccine status during that time? Remember that our ‘anchor’ is the index date, the date that someone tested. For our analysis, among cases, we will take the first positive event. For controls (test negative), we will randomly sample the index date/case. The random sampling will be non-differential between the vaccinated and unvaccinated.\nBut why not compare cases with controls that have the same index date? People with the same index date are comparable in terms of the day-to-day pandemic context that gave rise to their testing. What is being suggested here is that we match cases and controls with the same index date. The issue is that it might be hard to find a control for every case. But matching could be considered to enhance the comparability."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mary Aglipay",
    "section": "",
    "text": "PhD Epidemiology Student, Dalla Lana School of Public Health, University of Toronto"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mary Aglipay",
    "section": "",
    "text": "Defend!\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nControlled Direct Effects- What is going on?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 10, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBut wait, isn’t the effectiveness of masks known?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nConsequences of exclusion criteria in my cohort data\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhat is an interrupted time series analysis?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAssumptions of all the statistical models!\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPlease, what is the parametric g-formula?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhy not use a cohort study instead of a test-negative study? (and vice versa)\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is record-level quantitative bias analysis?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDecomposition and probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning - opportunities?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations of the data\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWho are your controls in test-negative studies?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Negative Studies: Let’s Talk About It\n\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\n\n\n\n\n  \n\n\n\n\nConsequences of loss to follow-up\n\n\n\n\n\n\n\n\n\n\n\n\nDec 9, 2012\n\n\n\n\n\n\nNo matching items"
  }
]