[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "maryaglipay.github.io",
    "section": "",
    "text": "This is a Quarto website. Hello!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Test Negative Studies: Let’s Talk About It",
    "section": "",
    "text": "If you’ve been keeping up with the Covid-19 vaccine literature, there may be a study design that sticks out to you: a test-negative study. Huh? What’s that?\nBefore we get into what a test negative study is, let’s think about what these vaccine effectiveness studies want to examine. The exposure in these studies is typically vaccination status (0,1,2 doses, typically with 0 as the referent category), and the outcome is SARS-CoV-2 infection. The question is, what is the real-world impact of vaccines on SARS-CoV-2 infection?\nBut back up–why do we need to know real-world effectiveness of vaccines? Don’t we have RCT data on how well the vaccines work? Yes, we do- but these RCTs typically have very stringent inclusion criteria (children have to be very healthy, etc.), and the families that self-select into these studies tend to be very affluent, well-connected, educated. Moreover, RCTs tend to have very regimented follow-up and dosing—families are reminded that they need to come in for their second dose, an RCT coordinator makes sure they receive it, vaccine administration is highly protocol-ized (vaccine expiration and storage are well-monitored). In the real world, it doesn’t always happen this way. You’ve got kids who are maybe not the healthiest–they might have chronic conditions like asthma. Families aren’t always rich and educated. Families may not come for their second dose, or may not come on time. Vaccine distribution is challenging, with changes in expiry and variable storage abilities. If we want to make inference to a population that deals with these system-level challenges, that has some health conditions, that are not as rich/well connected, then we need to do real-world effectiveness studies.\nOk, so how do we do this? In an ideal world, we could conduct a prospective cohort study—recruit children, follow them up over time, surveil them daily to see who got infected with SARS-CoV-2 and when. Sounds simple right? The problem is that this is really, really resource intensive. You need to follow a lot of children to have precise estimates. You need to test these children every day, and even if you tested them weekly, this can mean thousands upon thousands of PCR tests. You might have hard time finding families/children who are willing to do this.\nOk. So cohort studies are too expensive. What the next best thing? The next best thing might be a population-based case control study. Here we would have complete case ascertainment, and we would select a random sample of population controls. This study, in essence, could be viewed as nested within the cohort, with missing data (i.e. controls we did not select) as missing at random. In other words, according to Sullivan, this would be a cohort study in which the statum of people who do not test are ignored. This study would be much cheaper, as we wouldn’t have to do hundreds of thousands of PCR tests. But population controls are challenging—the way that they are ascertained might be different from the way cases as ascertained. The population controls might be very different than the cases. In our scenario, cases are ascertained through testing centres or pharmacies or other places that do PCR tests. These people might have better access to healthcare than the general population. This means the cases and conrols will be non-exchangeable–there are confounding factors that will impact case ascertainment as well as vaccination status, meaning there will be bias.\nOK. What about a case-control study with ‘other’ patient controls–i.e. people who can access the testing centres and pharmacies. Now we’re onto something. These controls are more like our cases—there is some indirect matching going on here. This brings us to the test-negative study. A test-negative study recruit participants who attend Covid assessment centres and test positive for Covid-19; controls are participants who undergo the same tests for the same reasons as the cases, but test negative.\nThe advantage of test-negative studies is that they have similar participation rates between cases and controls, similar information quality and completeness, and similar access. A particular advantage of test-negative studies is that they compare individuals with similar health seeking-behaviour and access to testing. We might imagine that a person who is more likely to seek vaccines is probably also more likely to find a way to get a PCR test. This is especially important given testing restrictions, which excluded many individuals from obtaining a test after December 2021. Residual confounding might also be less of a concern in test negative studies because (see Shi et al) variables that impact vaccination status are similar for Covid-19 and non-Covid-19 respiratory infections (e.g. children with parents who are healthcare workers might be more likely to get the vaccine, but the proportion of children with healthcare workers who have Covid-19 versus some other respiratory infection is probably the same).\nSome people have had been critical of test-negative designs. Westreich et al. state that test-negative designs are not really case-control studies as controls are not formally sampled from the source population. However, as Vandenbroucke and Pearce point out, there is a broad class of case control studies that use ‘other’ patient controls, which can sometimes help with exchangeability between the cases and controls. These designs using ‘other’ patient controls can provide valid estimates when (according to Jackson and Nelson) a) other respiratory viruses are not impacted by the vaccine and b) vaccine effectiveness does not vary by health-care seeking behaviour.\nOthers have asserted that a test-negative study is a cohort study with some incomplete follow-up. However, there is no follow-up happening in test negative designs because case and control status are ascertained at a single point in time: the time of the test (Vandenbrocke and Pearce).\nHowever, under the assumption of no confounding, selection bias, misclassification and assuming parametric assumptions are met, the test-negative study can provide valid estimates of vaccine effectiveness."
  },
  {
    "objectID": "posts/blog1/blog3.html",
    "href": "posts/blog1/blog3.html",
    "title": "Limitations of the data",
    "section": "",
    "text": "A question I get often when I’m doing a talk is, “what are the limitations of the data that you’re using?” And it’s a great question. We should always think what our data can tell us and what it cannot.\nOne of my first data sources is a large children’s cohort study. The strengths of this data is that it has rich, repeated measures about adherence to NPIs, about parent-reported infection, symptoms, sociodemographics, lifestyle, child health behaviours. It samples from primary care, which is great because we know that more than 95% of children access primary care in the first few years of life.\nHowever, it also comes with another limitations. I will frame these limitations in terms of the three types of bias: selection bias, measurement error, and confounding.\nSelection bias: Like many other cohorts, it suffers from selection processes that make it easier for richer, more educated, whiter participants to participate in the study. This may limit its external generalizability. Another limitation, in the some vein, is that richer, more educated families tend to remain in the study longer than those who are not. What does that mean? Well, if richer, whiter people are more likely to adhere to masks, and they are also less likely to get infected, and it would bias the resut away from the null.\nMeasurement error: Like other studies that use survey data, there is a certain amount of measurement error that comes along with questionnaires. For example, families might have trouble recalling how many days their child was adherent to masks. There may be some measurement error associated with self-report of rapid antigen tests, which in themselves have imperfect diagnostic accuracy, which then must get reported correctly by parents. Moreover, the tests must be administered properly, which may not occur.\nConfounding: One of the strengths of cohort studies is that they can collect really rich confounder information. For example, TARGet Kids! has great information not only on child NPI adherence, but they also have information on parent NPI adherence, household income, maternal and paternal education. However, there are measures that we have not collected–for example, as much as we can try to control for ‘health seeking behaviour’ through other measures (visits to the doctor, flu shot in past few months, perception about health), there may still may be some residual confounding.\nThe next data source are population health administrative data. This is data that is usually collected for billing and administrative purposes. Strengths of administrative data include the fact that it is population-wide–in our data, we can capture every single community dwelling child that obtained a vaccine in the province. This means we have a lot of data–a lot more than if we were to look at, for example, cohort data–which enables us to get more precise estimates. Administrative is also fast and efficient- data is recorded in real time, enabling us to get answers quickly. And lastly, administrative data is extremely resource-efficient; we do not require RAs, infrastructure to collect this data, as it is already being collected for administrative and financial purposes.\nBecause this data is not being collected for research purposes, it also comes with a number of drawbacks:\nSelection biases: Selection biases still exist will health administrative data (even though they do capture every encounter with the healthcare system), because they require participants to have access to the healthcare system. Unequal access is common, and in our case, it is exacerbated by government restriction on testing at the end of December 2021. In the third study, we are restricted to participants who were able to access testing for their child. For the most part, this included children of essential workers working (including physicians). However, although government restirictions were the same across Ontario, the manner in which they were implemented from hospital to hospital differed. This may mean that the external generalizability of our results to the general Ontario population may be limited.\nMeasurement error: Health administrative data can suffer from undercoding or miscoding of variables. It is unlikely for vaccinations to be miscoded, given the stringency of the vaccination protocol and the tracking of every single dose. PCR testing does not have perfect sensitivity and specificity, though it is considered the gold standard for testing for SARS-CoV-2.\nConfounding: Because this data is not collected for research purposes, there are a number of variables that may be important confounders for which we lack information. For example, many individual-level sociodemographics are not available–income, ethnicity, education. For our VE study, we use area-level sociodemographics as a proxy. These include neighbourhood income quintile, essential worker status quintile, and large household size quintile. These may or may not serve as good proxies for individual-level characteristics.\nThese data sources come with their limitations. However, they also come with a number of complementary strengths- the cohort has information that ICES lacks, and ICES has information that the cohort lacks, and it also covers many many children. It is important to leverage these strengths to make the most out of available data to answer important questions about SARS-CoV-2 in children."
  },
  {
    "objectID": "posts/blog1/blog2.html",
    "href": "posts/blog1/blog2.html",
    "title": "Who are your controls in test-negative studies?",
    "section": "",
    "text": "One thing that comes up in test-negative studies is, who exactly are the controls? Repeat after me: your controls are participants in your study who tested negative. If you’ve been working with cohort data or RCT data a lot, this is not intuitive: we often think in terms of exposed and unexposed. Controls are not your unexposed; they are those who tested negative.\nThe overall test effectiveness study will just compare those who test positive with those who test negative, full stop. [Note: you’ll still have to adjust for calendar date, because what might happen is that perhaps as the pandemic rolls on, as we change seasons, people are more likely to get vaccinated and also perhaps more likely to test positive. This biases the result toward the null]\nWe can do interval analysis too-e.g., what is the vaccine effectiveness, or impact of vaccines, on children who were vaccinated only 14-29 days prior to their index date (remember that the index date is the day that they take their test). Another way of phrasing it is, what is the vaccine effectiveness 14-29 days after vaccination?\nIn interval analysis, who are the controls? Again, they are still children who tested negative in that interval. How to know if they are in the interval? For vaccinated test-negatives (controls), you’ll have an ‘interval date’, which tells you how along ago vaccination happened. In our example, you’ll select vaccinated controls who vaccinated 14-29 days before their index date. For unvaccinated control, this interval date doesn’t really make any sense. From which date should we count the unvaccinated control (or case for that matter)?\nRemember our time zero is July 28, 2022 (when Ontario rolled out vaccines for children in this age group). We can calculate a ‘time interval’ for unvaccinated children between their index date and this date.\nTake for example, the stratified/interval analysis in children who tested 14-29 days ago. Let’s make a 2x2 table below.\n\n\n\n\n\n\n\n\n\nTest negative\nTest positive\n\n\n\n\nUnvaccinated\nUnvaccinated 14-29 days ago\nUnvaccinated 14-29 days ago.\n\n\nVaccinated\nVaccinated 14-29 days ago\nVaccinated 14-29 days ago\n\n\n\nWe should be looking at everyone 14-29 days prior to their index date, or 14-29 days ‘after vaccination’- but for the unvaccinated, ‘after vaccination’ doesn’t make sense because they were never vaccinated. But we can look at the interval between the roll-out date and their index date. As long as it was greater than 14-29 days, they can be considered. An index date that happened 90 days after roll-out for the unvaccinated can be included in the comparison of 14-29 days, because unvaccinated at 90 days also means unvaccinated at 60 days, and unvaccinated at 30 days. That means we can include everyone who is unvaccinated at an index date greater than the interval we are interested in, in our analysis, whether they are test negative or test positive.\nOther notes:\nLet’s talk about ‘follow-up,’ which is a bit of a tricky concept. Note that we can’t really talk about ‘incomplete’ or ‘complete’ follow-up in this analysis because outcome ascertainment happens at one point in time: the time of the test (see last paragraph in this blog post). But there is kind of a follow-up time for the vaccinated group, the time from vaccination to the index date; in the unvaccinated group, the ‘follow-up’ time is a bit arbitrary—it’s counted from time zero, date of vaccine rollout in Ontario. But remember, a person who is unvaccinated at their index date, which happens, for example, 90 days after time zero, is also unvaccinated at time 60, time 30.\nWhat about someone who with multiple tests–they tested negative at 1, then tested positive at another, and they changed their vaccine status during that time? Remember that our ‘anchor’ is the index date, the date that someone tested. For our analysis, among cases, we will take the first positive event. For controls (test negative), we will randomly sample the index date/case. The random sampling will be non-differential between the vaccinated and unvaccinated.\nBut why not compare cases with controls that have the same index date? People with the same index date are comparable in terms of the day-to-day pandemic context that gave rise to their testing. What is being suggested here is that we match cases and controls with the same index date. The issue is that it might be hard to find a control for every case. But matching could be considered to enhance the comparability."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mary Aglipay",
    "section": "",
    "text": "PhD Epidemiology Student, Dalla Lana School of Public Health, University of Toronto"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mary Aglipay",
    "section": "",
    "text": "Limitations of the data\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\nMachine learning - opportunities?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\n\n\n\n\n\n\nTest Negative Studies: Let’s Talk About It\n\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\n\n\n\n\n\n\nWho are your controls in test-negative studies?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog1/blog4.html",
    "href": "posts/blog1/blog4.html",
    "title": "Machine learning - opportunities?",
    "section": "",
    "text": "OK, so for one of my projects, I’m using some pretty complicated analyses–in particular, something called the parametric g-formula, which sounds a little crazy and spaceship-y. When you boil it down, all it’s doing is standardizing, which essentially means finding weighted averages across all of the counfounder strata.\nAlthough I wouldn’t exactly call it machine learning (what is machine learning, really, anyway?), the parametric g-formula is considered ‘advanced analytics’ because it requires quite a bit of computing power. In order to calculate those weighted averages, you need to need to run simulations so that you can get representation in each of those millions of combinations of confounder strata. And you simulate the covariate history, you simulate the censoring or outcome, for each of the many weekly timepoints in my analysis. This requires a ton of models. And, as we know, models each come with many assumptions- linearity, normality of the residuals, that the variance of the residuals is constant across the the observations (homoscedasticity), that there are no unduly influential points. When there are hundreds of models, model misspecification can produce a sizeable bias.\nWith the advent of machine learning methods, some of these assumptions might be relaxed. Machine learning can predict the probability of each variable at each time-step (see Blakely et al., 2020), potentially being made ’more robust to model misspecification through machine-learning techniques” (Westreich et al, 2015). These might include tree-based methods, random forests, and neural networks, for example.\nIt’s interesting because I think a lot of the field has been sort of bemoaning the fact that machine learning applications are restricted to prediction-type research questions–can we predict, for example, who will get covid, based on this set of covariates?\nBut even in causal inference methods, prediction can be part of the process. In the parametric g-formula, we actually need to predict the covariate history and outcome for a large, simulated dataset. We need not restrict ourselves to parametric models, though computationally, we may have increased challenges. I suspect in the future, with even greater computing power, we will see more machine learning methods being applied in the causal inference space (as we already have!) This is an area of growth in the field of epidemiology."
  }
]