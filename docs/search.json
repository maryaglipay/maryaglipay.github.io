[
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "maryaglipay.github.io",
    "section": "",
    "text": "This is a Quarto website. Hello!\nTo learn more about Quarto websites visit https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/blog1/index.html",
    "href": "posts/blog1/index.html",
    "title": "Test Negative Studies: Let’s Talk About It",
    "section": "",
    "text": "If you’ve been keeping up with the Covid-19 vaccine literature, there may be a study design that sticks out to you: a test-negative study. Huh? What’s that?\nBefore we get into what a test negative study is, let’s think about what these vaccine effectiveness studies want to examine. The exposure in these studies is typically vaccination status (0,1,2 doses, typically with 0 as the referent category), and the outcome is SARS-CoV-2 infection. The question is, what is the real-world impact of vaccines on SARS-CoV-2 infection?\nBut back up–why do we need to know real-world effectiveness of vaccines? Don’t we have RCT data on how well the vaccines work? Yes, we do- but these RCTs typically have very stringent inclusion criteria (children have to be very healthy, etc.), and the families that self-select into these studies tend to be very affluent, well-connected, educated. Moreover, RCTs tend to have very regimented follow-up and dosing—families are reminded that they need to come in for their second dose, an RCT coordinator makes sure they receive it, vaccine administration is highly protocol-ized (vaccine expiration and storage are well-monitored). In the real world, it doesn’t always happen this way. You’ve got kids who are maybe not the healthiest–they might have chronic conditions like asthma. Families aren’t always rich and educated. Families may not come for their second dose, or may not come on time. Vaccine distribution is challenging, with changes in expiry and variable storage abilities. If we want to make inference to a population that deals with these system-level challenges, that has some health conditions, that are not as rich/well connected, then we need to do real-world effectiveness studies.\nOk, so how do we do this? In an ideal world, we could conduct a prospective cohort study—recruit children, follow them up over time, surveil them daily to see who got infected with SARS-CoV-2 and when. Sounds simple right? The problem is that this is really, really resource intensive. You need to follow a lot of children to have precise estimates. You need to test these children every day, and even if you tested them weekly, this can mean thousands upon thousands of PCR tests. You might have hard time finding families/children who are willing to do this.\nOk. So cohort studies are too expensive. What the next best thing? The next best thing might be a population-based case control study. Here we would have complete case ascertainment, and we would select a random sample of population controls. This study, in essence, could be viewed as nested within the cohort, with missing data (i.e. controls we did not select) as missing at random. In other words, according to Sullivan, this would be a cohort study in which the statum of people who do not test are ignored. This study would be much cheaper, as we wouldn’t have to do hundreds of thousands of PCR tests. But population controls are challenging—the way that they are ascertained might be different from the way cases as ascertained. The population controls might be very different than the cases. In our scenario, cases are ascertained through testing centres or pharmacies or other places that do PCR tests. These people might have better access to healthcare than the general population. This means the cases and conrols will be non-exchangeable–there are confounding factors that will impact case ascertainment as well as vaccination status, meaning there will be bias.\nOK. What about a case-control study with ‘other’ patient controls–i.e. people who can access the testing centres and pharmacies. Now we’re onto something. These controls are more like our cases—there is some indirect matching going on here. This brings us to the test-negative study. A test-negative study recruit participants who attend Covid assessment centres and test positive for Covid-19; controls are participants who undergo the same tests for the same reasons as the cases, but test negative.\nThe advantage of test-negative studies is that they have similar participation rates between cases and controls, similar information quality and completeness, and similar access. A particular advantage of test-negative studies is that they compare individuals with similar health seeking-behaviour and access to testing. We might imagine that a person who is more likely to seek vaccines is probably also more likely to find a way to get a PCR test. This is especially important given testing restrictions, which excluded many individuals from obtaining a test after December 2021. Residual confounding might also be less of a concern in test negative studies because (see Shi et al) variables that impact vaccination status are similar for Covid-19 and non-Covid-19 respiratory infections (e.g. children with parents who are healthcare workers might be more likely to get the vaccine, but the proportion of children with healthcare workers who have Covid-19 versus some other respiratory infection is probably the same).\nSome people have had been critical of test-negative designs. Westreich et al. state that test-negative designs are not really case-control studies as controls are not formally sampled from the source population. However, as Vandenbroucke and Pearce point out, there is a broad class of case control studies that use ‘other’ patient controls, which can sometimes help with exchangeability between the cases and controls. These designs using ‘other’ patient controls can provide valid estimates when (according to Jackson and Nelson) a) other respiratory viruses are not impacted by the vaccine and b) vaccine effectiveness does not vary by health-care seeking behaviour.\nOthers have asserted that a test-negative study is a cohort study with some incomplete follow-up. However, there is no follow-up happening in test negative designs because case and control status are ascertained at a single point in time: the time of the test (Vandenbrocke and Pearce).\nHowever, under the assumption of no confounding, selection bias, misclassification and assuming parametric assumptions are met, the test-negative study can provide valid estimates of vaccine effectiveness."
  },
  {
    "objectID": "posts/blog1/blog5.html",
    "href": "posts/blog1/blog5.html",
    "title": "What is record-level quantitative bias analysis?",
    "section": "",
    "text": "In my proposal, there are a number of times when I mention that I am going to be doing record-level quantitative bias analysis. Though I am explicit about what the required bias parameters are, what will I do with those bias parameters after I’ve found them?\nIn the Fox and Lash book (2020), they list a couple of steps needed to do record level QBA. These are:\n\nAssign probability distributions to each of the bias parameters\nUse simple bias methods to generate bias-adjusted data to inform the bias analysis and apply bias parameters probabilistically\nSave bias adjusted estimate and repeat steps 4a to c\nSummarize bias adjusted estimates with a frequency distribution that yields a central tendancy and simulation interval\n\nLet’s go through them in detail.\n\n1. Assign probability distributions to each of the bias parameters\nOnce we have established the bias parameters for your analysis (in the case of outcome misclassification, it is SE, SP, PPV, NPV), first, it’s important to remember that there’s error around each of your bias parameters. Even though you found a single value from, for example, the literature, there is a range of plausible values for each bias parameter. We can assign a distribution of probabilities around the estimated parameter (e.g. trapezoidal, normal distribution, uniform distribution)\n\n\n2. Use simple bias analysis methods to incorporate uncertainty in the bias parameters and random error\n\n2a. Obtain using the bias parameters, calculate misclassification-adjusted sensitivity, specificity, PPV, NPV of the outcome for every record in the dataset.\n\nCreate a simple contingency table of your exposure-outcome relationship.\nUse Monte Carlo sampling techniques to select bias parameter values from the probability distribution in 1.\nUse the sampled bias parameter to correct the cells in the 2x2 exposure-outcome contingency table. Insert the values of the corrected cells as new variables for the first record.\nFrom these four variables, calculate sensitivity, specificity, NPV, PPV. Insert these values as new variables for the first record.\nRepeat ii) to iv) again for all the records in your dataset.\n\n\n\n2b. Reclassify the outcome, use reclassified outcome to obtain new estimate of association, and incorporate random error\n\nUse Bernoulli trials to use the values calculated in A to determine if the record had the outcome or was censored–the ‘reclassified’ outcome status.\nUse the reclassified outcome status in your new regression analyses.\nSimulate random error and incorporate it into your estimate.\n\n\n\n\n3. Save this estimate and repeat all steps above for X number of iterations.\n\n\n4. Summarize the distribution of bias-adjusted estimates using a simulation interval."
  },
  {
    "objectID": "posts/blog1/blog4.html",
    "href": "posts/blog1/blog4.html",
    "title": "Machine learning - opportunities?",
    "section": "",
    "text": "OK, so for one of my projects, I’m using some pretty complicated analyses–in particular, something called the parametric g-formula, which sounds a little crazy and spaceship-y. When you boil it down, all it’s doing is standardizing, which essentially means finding weighted averages across all of the counfounder strata.\nAlthough I wouldn’t exactly call it machine learning (what is machine learning, really, anyway?), the parametric g-formula is considered ‘advanced analytics’ because it requires quite a bit of computing power. In order to calculate those weighted averages, you need to need to run simulations so that you can get representation in each of those millions of combinations of confounder strata. And you simulate the covariate history, you simulate the censoring or outcome, for each of the many weekly timepoints in my analysis. This requires a ton of models. And, as we know, models each come with many assumptions- linearity, normality of the residuals, that the variance of the residuals is constant across the the observations (homoscedasticity), that there are no unduly influential points. When there are hundreds of models, model misspecification can produce a sizeable bias.\nWith the advent of machine learning methods, some of these assumptions might be relaxed. Machine learning can predict the probability of each variable at each time-step (see Blakely et al., 2020), potentially being made ’more robust to model misspecification through machine-learning techniques” (Westreich et al, 2015). These might include tree-based methods, random forests, and neural networks, for example.\nIt’s interesting because I think a lot of the field has been sort of bemoaning the fact that machine learning applications are restricted to prediction-type research questions–can we predict, for example, who will get covid, based on this set of covariates?\nBut even in causal inference methods, prediction can be part of the process. In the parametric g-formula, we actually need to predict the covariate history and outcome for a large, simulated dataset. We need not restrict ourselves to parametric models, though computationally, we may have increased challenges. I suspect in the future, with even greater computing power, we will see more machine learning methods being applied in the causal inference space (as we already have!) This is an area of growth in the field of epidemiology."
  },
  {
    "objectID": "posts/blog1/blog6.html",
    "href": "posts/blog1/blog6.html",
    "title": "Decomposition and probabilities",
    "section": "",
    "text": "Now to move onto something a bit different. Decomposition! Decomposition is the process of separting effects into an indirect and direct effects for mediation analysis.\nLong story short- but we want to obtain the ‘nested counterfactuals’–ie. what the expected value would be if the mediator had acted in a way differnt than the actual treatment received.\nLet’s start with direct-indirect decomposition.\nThe formula for this is TE = NIE + TDE\nWhere \\(TE=E[Y^{(1,M^1)}]-E[Y^{(0,M^0)}]\\) (both switches on minus both switches off)\nwhich, if we add and subtract this term, called the nested counterfactual: \\(E[Y^{(1,M^0)}]\\)\nturns into: \\(TE=E[Y^{(1,M^1)}]-E[Y^{(1,M^0)}] + E[Y^{(1,M^0)}]-E[Y^{(0,M^0)}]\\)\nGreat! Now how do you get each of these terms? Remember, each term is made up of an exposure set to a value, with a mediator taking on one of two values, 0 or 1 (because ‘M under A=1’ can be either 0 or 1). We need to take a weighted average of each of those scenarios.\nThe first and last terms are easy:\n\\(E[Y^{(1,M^1)}]=E[Y|A=1,M=1]*Pr(M=1|A=1) + E[Y|A=1,M=0]*Pr(M=0|A=1)\\)\nBasically, it’s expected value of Y when A is set to 1 and M is also 1 weighted by the probability that M would take the value of 1 when A is set to 1 PLUS the expected value of Y when A is set to 1 and M is 0, weighted by the probability that M would take the value of 0 when A is set to 1.\nThe nested counterfactual is tricky:\n\\(E[Y^{(1,M^0)}]=E[Y|A=1,M=1]*Pr(M=1|A=0)+E[Y|A=1,M=0]*Pr(M=0|A=0)\\)\nHere, A remains fixed at 1, but M acting as if A were 0 can take on two values, 0 or 1. The term \\(E[Y^{(1,M^0)}]\\) means we need to add up the probabilities of those two scenarios. Before, it was easy- we basically took a weighted average of the two possible values of the mediator. The weights were easy because it was the observed probability– the probability of M conditional on the exposure level we actually saw.\nThis time, which probability do we take? In the previous example, we were interested in M ‘as if A were equal to 1’. Well guess what? A is equal to 1, so we simply weight by the probabilities that we see M=1 or M=0 in the category of the actual exposure value, A=1. This time, remember we are interested in M ‘as if A were equal to 0’. Well, this time, our actual exposure value A is equal to 1, how can we see M ‘as if A were equal to 0’? No such group really exists. This is why we need to look to the other group, A=0, and see the probability that M would take on 1 or 0 in this group. and multiply this by the expected value of the a scenario we can see."
  },
  {
    "objectID": "posts/blog1/blog7.html",
    "href": "posts/blog1/blog7.html",
    "title": "Why not use a cohort study instead of a test-negative study? (and vice versa)",
    "section": "",
    "text": "When we see vaccine effectiveness studies, we often see two types: test-negative studies and cohort studies.\nCohort studies are really nice because they enable us to obtain the relative risk, which is arguably more readily interpretable than the odds ratio. They can also establish the prevalence of a disease in a population.\nTest-negative studies have the advantage of being relatively cheaper and more readily conducted. This is not really true when we talk about population health administrative data, where the data is collected prospectively no matter what.\nSo why would we choose to do a test-negative study instead of a cohort study, or a case-control study for that matter?\nThere are two mains reasons(Jackson and Nelson 2013) are misclassification bias and confounding.\nIn a perfect world, we could fully capture vaccination, infection, and healthcare seeking behaviour in our population, we could make this 2x2 table:\n\n\n\nFrom Jackson and Nelson (2013)\n\n\nAnd the estimand here is (in the absence of effect modification by care-seeking behaviour):\n\nThe only way we know if someone got Covid-19 is if they got a PCR test. So our outcome is ‘tested positive for Covid-19’. When we do a cohort study, our ‘did not test positive for Covid-19’ group is sampled from B, C, D, E, F for the vaccinated and the ‘did not test positive for Covid-19’ group is sampled from H, I, J, K and L for the unvaccinated group. Similarly, when we do a case-control study, we sample controls from B,C,D,E,F and H,I,J,K,L.\nThis will lead to biased results from misclassification because group D and group J, the infected with influenza but did not seek PCR testing, will be misclassified as ‘not infected’ in the cohort study, and as controls in the case-control study. This will likely bias the results toward the null, as those who do not vaccinate are also less likely to test, and more likely to appear ‘not infected’.\nThere is also bias from healthcare-seeking behaviour because those with a higher healthcare seeking behaviour propensity will be more likely to vaccinate and also more likely to seek testing.\n\nHow does a test-negative study avoid misclassification bias and confounding bias?\nBy restricting on those who test, one no longer needs to sample from the ‘not care seeking’ population, so misclassification that exists from including those who are actually positive but did not test, is no longer an issue.\nAlso, by restricting on those who test, we block the backdoor path that can give rise to biased estimates.\n\nBest of all, we can still get valid estimates of VE (see below), though our ability to generalize to the not tested is limited.\n\n\n\nBut wait! Doesn’t your other study use a cohort design?\nWhy yes it does! Thanks for asking. It would be challenging to do a test-negative study with our cohort data because we would have a limited sample size to find an effect.\nWe are trying to mitigate misclassification by including tests reported by parents via rapid antigen tests. We also have an extensive quantitative bias analysis planned for misclassification outcome by using record-level probabilistic QBA processes.\nWe will try to mitigate confounding by health-seeking behaviour by adjusting for visits to the family doctor, and other indicators of health-seeking behaviour including education, income, ethnicity, and attitudes toward vaccines.\n\n\n\n\n\nReferences\n\nJackson, Michael L., and Jennifer C. Nelson. 2013. “The Test-Negative Design for Estimating Influenza Vaccine Effectiveness.” Vaccine 31 (17): 2165–68."
  },
  {
    "objectID": "posts/blog1/blog3.html",
    "href": "posts/blog1/blog3.html",
    "title": "Limitations of the data",
    "section": "",
    "text": "A question I get often when I’m doing a talk is, “what are the limitations of the data that you’re using?” And it’s a great question. We should always think what our data can tell us and what it cannot.\nOne of my first data sources is a large children’s cohort study. The strengths of this data is that it has rich, repeated measures about adherence to NPIs, about parent-reported infection, symptoms, sociodemographics, lifestyle, child health behaviours. It samples from primary care, which is great because we know that more than 95% of children access primary care in the first few years of life.\nHowever, it also comes with another limitations. I will frame these limitations in terms of the three types of bias: selection bias, measurement error, and confounding.\nSelection bias: Like many other cohorts, it suffers from selection processes that make it easier for richer, more educated, whiter participants to participate in the study. This may limit its external generalizability. Another limitation, in the some vein, is that richer, more educated families tend to remain in the study longer than those who are not. What does that mean? Well, if richer, whiter people are more likely to adhere to masks, and they are also less likely to get infected, and it would bias the resut away from the null.\nMeasurement error: Like other studies that use survey data, there is a certain amount of measurement error that comes along with questionnaires. For example, families might have trouble recalling how many days their child was adherent to masks. There may be some measurement error associated with self-report of rapid antigen tests, which in themselves have imperfect diagnostic accuracy, which then must get reported correctly by parents. Moreover, the tests must be administered properly, which may not occur.\nConfounding: One of the strengths of cohort studies is that they can collect really rich confounder information. For example, TARGet Kids! has great information not only on child NPI adherence, but they also have information on parent NPI adherence, household income, maternal and paternal education. However, there are measures that we have not collected–for example, as much as we can try to control for ‘health seeking behaviour’ through other measures (visits to the doctor, flu shot in past few months, perception about health), there may still may be some residual confounding.\nThe next data source are population health administrative data. This is data that is usually collected for billing and administrative purposes. Strengths of administrative data include the fact that it is population-wide–in our data, we can capture every single community dwelling child that obtained a vaccine in the province. This means we have a lot of data–a lot more than if we were to look at, for example, cohort data–which enables us to get more precise estimates. Administrative is also fast and efficient- data is recorded in real time, enabling us to get answers quickly. And lastly, administrative data is extremely resource-efficient; we do not require RAs, infrastructure to collect this data, as it is already being collected for administrative and financial purposes.\nBecause this data is not being collected for research purposes, it also comes with a number of drawbacks:\nSelection biases: Selection biases still exist will health administrative data (even though they do capture every encounter with the healthcare system), because they require participants to have access to the healthcare system. Unequal access is common, and in our case, it is exacerbated by government restriction on testing at the end of December 2021. In the third study, we are restricted to participants who were able to access testing for their child. For the most part, this included children of essential workers working (including physicians). However, although government restirictions were the same across Ontario, the manner in which they were implemented from hospital to hospital differed. This may mean that the external generalizability of our results to the general Ontario population may be limited.\nMeasurement error: Health administrative data can suffer from undercoding or miscoding of variables. It is unlikely for vaccinations to be miscoded, given the stringency of the vaccination protocol and the tracking of every single dose. PCR testing does not have perfect sensitivity and specificity, though it is considered the gold standard for testing for SARS-CoV-2.\nConfounding: Because this data is not collected for research purposes, there are a number of variables that may be important confounders for which we lack information. For example, many individual-level sociodemographics are not available–income, ethnicity, education. For our VE study, we use area-level sociodemographics as a proxy. These include neighbourhood income quintile, essential worker status quintile, and large household size quintile. These may or may not serve as good proxies for individual-level characteristics.\nThese data sources come with their limitations. However, they also come with a number of complementary strengths- the cohort has information that ICES lacks, and ICES has information that the cohort lacks, and it also covers many many children. It is important to leverage these strengths to make the most out of available data to answer important questions about SARS-CoV-2 in children."
  },
  {
    "objectID": "posts/blog1/blog2.html",
    "href": "posts/blog1/blog2.html",
    "title": "Who are your controls in test-negative studies?",
    "section": "",
    "text": "One thing that comes up in test-negative studies is, who exactly are the controls? Repeat after me: your controls are participants in your study who tested negative. If you’ve been working with cohort data or RCT data a lot, this is not intuitive: we often think in terms of exposed and unexposed. Controls are not your unexposed; they are those who tested negative.\nThe overall test effectiveness study will just compare those who test positive with those who test negative, full stop. [Note: you’ll still have to adjust for calendar date, because what might happen is that perhaps as the pandemic rolls on, as we change seasons, people are more likely to get vaccinated and also perhaps more likely to test positive. This biases the result toward the null]\nWe can do interval analysis too-e.g., what is the vaccine effectiveness, or impact of vaccines, on children who were vaccinated only 14-29 days prior to their index date (remember that the index date is the day that they take their test). Another way of phrasing it is, what is the vaccine effectiveness 14-29 days after vaccination?\nIn interval analysis, who are the controls? Again, they are still children who tested negative in that interval. How to know if they are in the interval? For vaccinated test-negatives (controls), you’ll have an ‘interval date’, which tells you how along ago vaccination happened. In our example, you’ll select vaccinated controls who vaccinated 14-29 days before their index date. For unvaccinated control, this interval date doesn’t really make any sense. From which date should we count the unvaccinated control (or case for that matter)?\nRemember our time zero is July 28, 2022 (when Ontario rolled out vaccines for children in this age group). We can calculate a ‘time interval’ for unvaccinated children between their index date and this date.\nTake for example, the stratified/interval analysis in children who tested 14-29 days ago. Let’s make a 2x2 table below.\n\n\n\n\n\n\n\n\n\nTest negative\nTest positive\n\n\n\n\nUnvaccinated\nUnvaccinated 14-29 days ago\nUnvaccinated 14-29 days ago.\n\n\nVaccinated\nVaccinated 14-29 days ago\nVaccinated 14-29 days ago\n\n\n\nWe should be looking at everyone 14-29 days prior to their index date, or 14-29 days ‘after vaccination’- but for the unvaccinated, ‘after vaccination’ doesn’t make sense because they were never vaccinated. But we can look at the interval between the roll-out date and their index date. As long as it was greater than 14-29 days, they can be considered. An index date that happened 90 days after roll-out for the unvaccinated can be included in the comparison of 14-29 days, because unvaccinated at 90 days also means unvaccinated at 60 days, and unvaccinated at 30 days. That means we can include everyone who is unvaccinated at an index date greater than the interval we are interested in, in our analysis, whether they are test negative or test positive.\nOther notes:\nLet’s talk about ‘follow-up,’ which is a bit of a tricky concept. Note that we can’t really talk about ‘incomplete’ or ‘complete’ follow-up in this analysis because outcome ascertainment happens at one point in time: the time of the test (see last paragraph in this blog post). But there is kind of a follow-up time for the vaccinated group, the time from vaccination to the index date; in the unvaccinated group, the ‘follow-up’ time is a bit arbitrary—it’s counted from time zero, date of vaccine rollout in Ontario. But remember, a person who is unvaccinated at their index date, which happens, for example, 90 days after time zero, is also unvaccinated at time 60, time 30.\nWhat about someone who with multiple tests–they tested negative at 1, then tested positive at another, and they changed their vaccine status during that time? Remember that our ‘anchor’ is the index date, the date that someone tested. For our analysis, among cases, we will take the first positive event. For controls (test negative), we will randomly sample the index date/case. The random sampling will be non-differential between the vaccinated and unvaccinated.\nBut why not compare cases with controls that have the same index date? People with the same index date are comparable in terms of the day-to-day pandemic context that gave rise to their testing. What is being suggested here is that we match cases and controls with the same index date. The issue is that it might be hard to find a control for every case. But matching could be considered to enhance the comparability."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Mary Aglipay",
    "section": "",
    "text": "PhD Epidemiology Student, Dalla Lana School of Public Health, University of Toronto"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Mary Aglipay",
    "section": "",
    "text": "Decomposition and probabilities\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLimitations of the data\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMachine learning - opportunities?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 30, 2022\n\n\n\n\n\n\n  \n\n\n\n\nPlease, what is the parametric g-formula?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTest Negative Studies: Let’s Talk About It\n\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat is record-level quantitative bias analysis?\n\n\n\n\n\n\n\n\n\n\n\n\nDec 2, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWho are your controls in test-negative studies?\n\n\n\n\n\n\n\n\n\n\n\n\nNov 29, 2022\n\n\n\n\n\n\n  \n\n\n\n\nWhy not use a cohort study instead of a test-negative study? (and vice versa)\n\n\n\n\n\n\n\n\n\n\n\n\nDec 4, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blog1/blog 8.html",
    "href": "posts/blog1/blog 8.html",
    "title": "Please, what is the parametric g-formula?",
    "section": "",
    "text": "The parametric g-formula has terrible branding. The name sounds like some kind of rocket booster requiring advanced-level physics to operate it.\nAnd although it may sounds complicated, it really comes to thinking about it as being akin to standardization. In epi 101, we learn that standardization really means finding the weighted averages that make two groups comparable (if we think about clinical trials, the whole point of randomizing people into two groups is to make them more comparable- this is called exchangeability). For example, say we want to see if living in country A causes cancer, compared to living in country B. But say we know that age and sex are associated with cancer (e.g. older individuals and women are more likely to have this kind of cancer), and country A just happens to have a high proportion of women and older individuals. What we want to do is weight the groups in a way that they have similar distributions of women and older people, so that they are more comparable. The general formula for a standardized estimate of Y is\n\n\n\nwhere \\(A\\) is the level of exposure, \\(C\\) is censoring, and \\(L\\) are the level of covariates.\n\n\nWhen there aren’t many variables, we can get this non-parametrically (i.e. we don’t need to use models!). Basically, what we need to do is find the mean value of Y in each of the confounder strata (that’s the conditional means, the \\(E[Y|A=a,C=0,L=l]\\) part), and weight it (multiply it) by the probability of being in that stratum (that’s the \\(Pr[L=l]\\) part. So, in our mortality example, let’s say we only had two age groups: young and old. We would find the mean value of \\(Y\\) in young women and multiply it by the probability of being a young woman, the mean value of \\(Y\\) in old women and multiply it by the probability of being an old woman, the mean value of \\(Y\\) in young men and multiply it by the probability of being a young man, and the mean value of \\(Y\\) in old men and multiply it by the probability of being a young man. Then we would add up all these terms, and voila, there’s your standardized risk of mortality for that specific country.\nNow say we have a ton of variables- sex, age, education, income, comorbidities, children, marital status, etc. It’s going to be really hard to do this non-parametrically because there’s simply not enough data to fill the thousands of strata created by every combination of confounders.\nWe need to resort to modelling. What we can do is develop a regression, conditioning on the confounders! After all, what a regression is really trying to do is model the $E[Y|A=a, C=0, L=l]$. Fantastic! Then what you do is using your data, you set \\(A\\) to equal the value of country A. You find the predictions.\nNow, you say-should we weight by the probability of being in that stratum? Again, it’s hard to find this when not every stratum is filled (it’s asking your data- what’s the probability of being an old woman, with this amoung of education, this income, this particular comorbidity, 6 children, unmarried, etc). Fortunately, there’s a fun trick- the weighted mean can be written as the double expectation. That is,\n\\(\\sum_l E[Y|A=a, C=0, L=l] \\times Pr(L=l)= E[E[Y|A=a, C=0, L=l] \\times Pr(L=l)]\\)\nWhat! What does this mean- it means we can find the mean of the conditional means and this ends up being your standardized mean! What! Amazing. So in summary, there are three steps:\n\nModel the conditional mean\nPredict the value of the conditional means for all of your confounder strata combinations.\nFind the mean of the conditional means in each group; find the risk difference, risk ratio, etc.\n\nThe parametric g-formula for time varying-exposures takes it a step further by modelling both the outcome and each of the time-varying covariates at each time point, and then applying simulations. Why might we need simulations? Well, if we were solely to rely on our dataset for step 2, we might not have people who fulfill all of those combinations of confounder strata; so we wouldn’t be able to estimate the sum of the conditional means.\nHow do you do a simulation?\nYou use something called Monte Carlo simulations–so based on your models from Step 1, you get predicted probabilities of Y, based on your covariates. Then you flip biased coins to determine which value a person will get.\nThis is where it gets interesting. THe first simulated dataset you’ll make is basically like your original dataset, except with far more people. But nothing’s been done, you haven’t altered your exposure in any way. This is the ‘natural course’ dataset. The second dataset however, whenever you simulate your time varying exposure, whatever value you get, you’ll increase that value by ONE DAY. This is the ‘increased by 1 day dataset’.\nThen, you predict the value of your conditional means using your simulated dataset–this time, you should have enough people in every combination of your confounder strata.\nThen, you find the mean of the conditional means for the ‘natural course’ dataset. And you find the mean of the conditional means for the ‘increased by one day’ dataset. If you subtract these, you will have the marginal mean difference! Wow!\nFurther recommended reading is available in Hernán and Robins (n.d.) and Keil et al. (2014).\n\n\n\n\nReferences\n\nHernán, Miguel A, and James M Robins. n.d. “Causal Inference: What If,” 311.\n\n\nKeil, Alexander P., Jessie K. Edwards, David R. Richardson, Ashley I. Naimi, and Stephen R. Cole. 2014. “The Parametric g-Formula for Time-to-Event Data: Towards Intuition with a Worked Example.” Epidemiology (Cambridge, Mass.) 25 (6): 889–97. https://doi.org/10.1097/EDE.0000000000000160."
  }
]